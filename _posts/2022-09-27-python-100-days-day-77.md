---
layout: post
title: Python 100 Days Day77 Introduction to Probability Statistics
categories: Dev
tags: [beginner, python]
---

author: [jackfrued](https://github.com/jackfrued/Python-100-Days)

## 概率統計基礎

概率論源於賭博遊戲。意大利文藝覆興時代，百科全書式的學者卡爾達諾（死後）發表的《論賭博遊戲》被認為是第一部論述概率論的著作。到了17世紀的法國，宮廷貴族里盛行著擲骰子遊戲，遊戲規則是玩家連續擲4次骰子，如果其中沒有6點出現，玩家贏，如果出現一次6點，則莊家（相當於現在的賭場）贏；後來為了使遊戲更刺激，遊戲規則發生了些許變化，玩家用2個骰子連續擲24次，不同時出現2個6點，玩家贏，否則莊家贏。在這樣的時代背景下，法國數學家帕斯卡和費馬創立了概率論，後來雅各布·伯努利發現，概率論遠遠不止用於賭博，他將他的思考和研究記錄下來，寫成了《猜度數》一書，提出了大數定理（**在一個隨機事件中，隨著試驗次數的增加，事件發生的頻率越趨近於一個穩定值**），這個定理在當時的保險公司得到了充分利用。

<!-- more -->

> **思考**：回到剛才的骰子遊戲，按照舊玩法，莊家獲勝的概率是多少？換成新玩法，莊家獲勝的概率與之前的玩法相同嗎？

以概率論為基礎的統計學顯然比概率論出現的時間更晚，而且一直以來都是一種尷尬的存在，處於各種鄙視鏈的底端。從數學的角度看，統計學中的數學原理過於膚淺；從應用科學的角度看，統計學太數學，跟應用沾不上邊。盧瑟福（原子核物理學之父）曾經說過，“如果你的實驗需要統計學，那麽你應該再做一個更好的實驗”；波普爾（20世紀最偉大的哲學家之一）也曾經對歸納邏輯進行過強烈的抨擊。曾經，由於歐幾里得、笛卡爾帶給人們的完美體系實在太過迷人，導致很多人都忽視了統計思維這一重要的科學思維方式。但是最近十年時間，隨著大數據和人工智能時代的來臨，統計學又以驚人的速度流行起來，因為大數據時代已經充分證明了經驗主義、歸納推理的強大之處；而人工智能實際上也是大數據加上深度學習的歸納方法所取得的成功。有了統計學，我們能夠處有效的處理海量的數據，也能夠正確理解數據分析的結果。

按照統計方法的不同，我們可以將統計學分類為描述統計學、推斷統計學、貝葉斯統計學等。描述統計學是用來描繪或總結觀察量的基本情況的統計方法，通常會將整理後的數據做成表格或圖表，具體包括數據的集中趨勢分析、離散趨勢分析和相關分析。推斷統計學是研究如何根據樣本數據推斷總體數據特征的方法，在無法獲得全量數據的情況下，推斷統計就是最為行之有效的方法。貝葉斯統計學的基礎是貝葉斯定理，貝葉斯定理將經驗和直覺與概率相關聯，和人類大腦的判斷原理十分類似，簡單的說就是在獲取到新的數據之後，先前憑借經驗和直覺獲得的概率是可以改變的。

### 數據和數據的分類

在統計學中，通過試驗、觀察、調查等獲得的材料被稱為數據，數據大致可以分為定性數據和定量數據，其中定性數據又可以分為定類尺度和定序尺度，定量數據又可以分為定距尺度和定比尺度，如下表所示。

<img src="https://github.com/jackfrued/mypic/raw/master/20220320232245.png" style="zoom:50%">

####定性數據的處理

1. 定類尺度（名義尺度）：定類尺度通常會處理成虛擬變量（啞變量），多個不同的類型最終會變成一個虛擬變量矩陣。
2. 定序尺度：定序尺度可以處理成一個序號，並通過該序號表示等級的高低。

#### 定量數據的處理

1. 線性歸一化
    $$ x_i' = \frac {x_i - min(X)} {max(X) - min(X)} $$

2. 零均值歸一化
    $$ x_i' = \frac {x_i - \mu} {\sigma} $$

### 數據的集中趨勢

我們經常會使用以下幾個指標來描述一組數據的集中趨勢：

1. 均值 - 均值代表某個數據集的整體水平，我們經常提到的客單價、平均訪問時長、平均配送時長等指標都是均值。均值是對數據進行概括的一個強有力的方法，將大量的數據濃縮成了一個數據。均值的缺點是容易受極值的影響，可以使用加權平均值或去尾平均值來消除極值的影響；對於正數可以用幾何平均值來替代算術平均值。
    - 算術平均值：$$\bar{x} = \frac{\sum_{i=1}^{n} {x_{i}}} {n} = \frac{x_{1}+x_{2}+\cdots +x_{n}}{n}$$，例如計算最近30天日均DAU、日均新增訪客等，都可以使用算術平均值。
    - 幾何平均值：$$\left(\prod_{i=1}^{n}x_{i}\right)^{\frac{1}{n}}={\sqrt[{n}]{x_{1}x_{2} \cdots x_{n}}}$$，例如計算不同渠道的平均轉化率、不同客群的平均留存率、不同品類的平均付費率等，就可以使用幾何平均值。
2. 中位數 - 將數據按照升序或降序排列後位於中間的數，它描述了數據的中等水平。中位數的計算分兩種情況：
    - 當數據體量$n$為奇數時，中位數是位於$\frac{n + 1}{2}$位置的元素。
    - 當數據體量$n$為偶數時，中位數是位於$\frac{n}{2}$和${\frac{n}{2}+1}$兩個位置元素的均值。
3. 眾數 - 數據集合中出現頻次最多的數據，它代表了數據的一般水平。一般在數據量比較大時，眾數才有意義，而且數據越集中，眾數的代表性就越好。眾數不受極值的影響，但是無法保證唯一性和存在性。

例子：有A和B兩組數據。

```
A組：5, 6, 6, 6, 6, 8, 10
B組：3, 5, 5, 6, 6, 9, 12
```

A組的均值：6.74，中位數：6，眾數：6。

B組的均值：6.57，中位數：6，眾數：5, 6。

> **說明**：在Excel中，可以使用`AVERAGE`、`GEOMEAN`、`MEDIAN`、`MODE.SNGL`、`MODE.MULT`函數分別計算均值、中位數和眾數。求中位數也可以使用`QUARTILE.EXC`或`QUARTILE.INC`函數，將第二個參數設置為2即可。

對A組的數據進行一些調整。

```
A組：5, 6, 6, 6, 6, 8, 10, 500
B組：3, 5, 5, 6, 6, 9, 12
```

A組的均值會大幅度提升，但中位數和眾數卻沒有變化。

|        | 優點                             | 缺點                                 |
| ------ | -------------------------------- | ------------------------------------ |
| 均值   | 充分利用了所有數據，適應性強     | 容易收到極端值（異常值）的影響       |
| 中位數 | 能夠避免被極端值（異常值）的影響 | 不敏感                               |
| 眾數   | 能夠很好的反映數據的集中趨勢     | 有可能不存在（數據沒有明顯集中趨勢） |

### 數據的離散趨勢

如果說數據的集中趨勢，說明了數據最主要的特征是什麽；那麽數據的離散趨勢，則體現了這個特征的穩定性。簡單的說就是數據越集中，均值的代表性就越強；數據波動越大，均值的代表性就越弱。

1. 極值：就是最大值（maximum）、最小值（minimum），代表著數據集的上限和下限。

    > **說明**：Excel 中，計算極值的函數分別是`MAX`和`MIN`。

2. 極差：又稱“全距”，是一組數據中的最大觀測值和最小觀測值之差，記作$R$。一般情況下，極差越大，離散程度越大，數據受極值的影響越嚴重。

3. 四分位距離：$\small{IQR = Q_3 - Q_1}$。

    > **提示**：箱線圖。

4. 方差：將每個值與均值的偏差進行平方，然後除以總數據量得到的值。簡單來說就是表示數據與期望值的偏離程度。方差越大，就意味著數據越不穩定、波動越劇烈，因此代表著數據整體比較分散，呈現出離散的趨勢；而方差越小，意味著數據越穩定、波動越平滑，因此代表著數據整體比較集中。簡單的總結一下，
    - 總體方差：$$ \sigma^2 = \frac {\sum_{i=1}^{N} {(X_i - \mu)^2}} {N} $$。
    - 樣本方差：$$ S^2 = \frac {\sum_{i=1}^{N} {(X_i - \bar{X})^2}} {N-1} $$。

    > **說明**：Excel 中，計算總體方差和樣本方差的函數分別是`VAR.P`和`VAR.S`。

5. 標準差：將方差進行平方根運算後的結果，與方差一樣都是表示數據與期望值的偏離程度。
    - 總體標準差：$$ \sigma = \sqrt{\frac{\sum_{i=1}^{N} {(X_i - \mu)^2}} {N}} $$
    - 樣本標準差：$$ S = \sqrt{\frac{\sum_{i=1}^{N} {(X_i - \bar{X})^2}} {N-1}} $$

    > **說明**：Excel 中，計算標準差的函數分別是`STDEV.P`和`STDEV.S`。

### 數據的頻數分析

用一定的方式將數據分組，然後統計每個分組中樣本的數量，再輔以圖表（如直方圖）就可以更直觀的展示數據分布趨勢的一種方法。

頻數分析的業務意義：

1. 大問題變小問題，迅速聚焦到需要關注的群體。
2. 找到合理的分類機制，有利於長期的數據分析（維度拆解）。

例如：一個班有50個學生，考試成績如下所示：

```
87,  80,  79,  78,  55,  80,  81,  60,  78,  82,  67,  74,  67, 74,  66,  91,  100,  70,  82,  71,  77,  94,  75,  83,  85,  84, 47,  75,  84,  96,  53,  86,  86,  89,  71,  76,  75,  80,  70,  83,  77,  91,  90,  82,  74,  74,  78,  53,  88,  72
```

在獲得數據後，我們先解讀數據的集中趨勢和離散趨勢。

均值：`77.4`，中位數：`78.0`，眾數：`74`。

最高分：`100`，最低分：`47`，極差：`53`，方差：`120.16`。

但是，僅僅依靠上面的指標是很難對一個數據集做出全面的解讀，我們可以把學生按照考試成績進行分組，如下所示。

| 分數段   | 學生人數 |
| -------- | -------- |
| <60      | 4        |
| [60, 65) | 1        |
| [65, 70) | 3        |
| [70, 75) | 9        |
| [75, 80) | 10       |
| [80, 85) | 11       |
| [85, 90) | 6        |
| [90, 95) | 4        |
| >=95     | 2        |

我們可以利用直方圖來查看數據分布的形態，對數據分布形態的測度主要以正態分布為標準進行衡量，正態分布在坐標軸上的形狀是一個鈴鐺型（鐘型），正態曲線以均值為中心左右對稱，如下圖所示，而上面的學生考試成績數據就呈現出正態分布的輪廓。

<img src="https://github.com/jackfrued/mypic/raw/master/20210716155507.png" width="80%">

我們可以數據分布的直方圖擬合出一條曲線與正態曲線進行比較，主要比較曲線的尖峭程度和對稱性，通常稱之為峰度和偏態。數據分布的不對稱性稱為偏態，偏態又分為正偏（右偏）或負偏（左偏）兩種。在正態分布的情況下，中位數和均值應該都在對稱軸的位置，如果中位數在左邊，均值在右邊，那麽數據的極端值也在右邊，數據分布曲線向右延伸，就是我們說的右偏；如果均值在左邊，中位數在右邊，那麽數據的極端值在左邊，數據分布曲線向左延伸，就是我們說的左偏。測定偏態的指標是偏態系數，Excel 中計算偏度系數使用的公式如下所示：
$$
SK = \frac{n}{(n - 1)(n - 2)} \sum(\frac{x_i - \bar{x}}{s})^3
$$
$\small{SK > 0}$時，分布呈現正偏，SK值越大，正偏程度越高。

$\small{SK < 0}$時，分布呈現負偏，SK值越小，負偏程度越高。

峰度是指數據分布的尖峭程度，一般可以表現為尖頂峰度、平頂峰度和標準峰度（正態分布的峰度）。測定峰度的指標是峰度系數，Excel 中計算峰度系數使用的公式如下所示：
$$
K = \frac{n(n + 1)}{(n - 1)(n - 2)(n - 3)}\sum(\frac{x_i - \bar{x}}{s})^4-\frac{3(n - 1)^2}{(n - 2)(n - 3)}
$$
峰度系數$\small{K < 0}$時，分布與正態分布相比更為扁平、寬肩、瘦尾；峰度系數$\small{K > 0}$時，分布與正態分布相比更為尖峰、瘦肩、肥尾。

### 數據的概率分布

#### 基本概念

1. 隨機現象：在一定條件下可能發生也可能不發生，結果具有偶然性的現象。

2. 樣本空間（*sample space*）：隨機現象一切可能的結果組成的集合。

  - 拋一枚硬幣的樣本空間：$\Omega = \{ \omega_1, \omega_2 \}$。
  - 拋兩枚硬幣的樣本空間：$\Omega = \{ \omega_1, \omega_2, \omega_3, \omega_4 \}$，其中$\omega_1 = (H, H)$，$\omega_2 = (H, T)$，$\omega_3 = (T, H)$，$\omega_4 = (T, T)$。
  - 離散型的樣本空間的元素是可列的，連續型的樣本空間的元素是（無限）不可列的。

3. 隨機試驗（*trials*）：在相同條件下對某種隨機現象進行觀測的試驗。隨機試驗滿足三個特點：

    - 可以在相同條件下重覆的進行。

    - 每次試驗的結果不止一個，事先可以明確指出全部可能的結果。

    - 重覆試驗的結果以隨機的方式出現（事先不確定會出現哪個結果）。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220322075000.png" style="zoom:75%">

4. 隨機變量（*random variable*）：如果$X$指定給概率空間$S$中每一個事件$e$有一個實數$X(e)$，同時針對每一個實數$r$都有一個事件集合$A_r$與其相對應，其中$A_r=\{e: X(e) \le r\}$，那麽$X$被稱作隨機變量。從這個定義看出，$X$的本質是一個實值函數，以給定事件為自變量的實值函數，因為函數在給定自變量時會產生因變量，所以將$X$稱為隨機變量。簡單的說，隨機變量的值需要通過試驗來確認。

    - 離散型隨機變量：數據可以一一列出。
    - 連續型隨機變量：數據不可以一一列出。

    > **說明**：如果離散型隨機變量的取值非常龐大時，可以近似看做連續型隨機變量。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220322075148.png" style="zoom:50%;">

    <img src="https://github.com/jackfrued/mypic/raw/master/20220322075331.png" style="zoom:50%;">

5. 概率（*probability*）：用一個0~1之間的數字表示隨機現象發生的可能性，也就是說概率是隨機事件出現可能性的度量。

6. 概率質量函數/概率密度函數：概率質量函數是描述離散型隨機變量為特定取值的概率的函數，通常縮寫為**PMF**。概率密度函數是描述連續型隨機變量在某個確定的取值點可能性的函數，通常縮寫為**PDF**。二者的區別在於，概率密度函數本身不是概率，只有對概率密度函數在某區間內進行積分後才是概率。

7. 隨機變量的數字特征：

    - （數學）期望：隨機變量按照概率的加權平均，它表示了概率分布的中心位置，反映隨機變量平均取值的大小。

        對於離散型隨機變量$ X $，若$ \sum_{i=1}^{\infty} x_ip_i $收斂，那麽它就是隨機變量$ X $的期望，記為$ E(X) $，即$ E(X) = \sum_{i=1}^{\infty} x_ip_i $，否則隨機變量$ X $的期望不存在。

        對於連續型隨機變量$ X $，其概率密度函數為$ f(x) $，若$ \int_{-\infty}^{\infty}xf(x)dx $收斂，則稱$ E(x) =  \int_{-\infty}^{\infty}xf(x)dx $為隨機變量$ X $的數學期望，否則隨機變量$ X $的期望不存在。

    - 方差：方差用來表示隨機變量概率分布的離散程度，對於隨機變量$ X $，若$ E((X - E(X))^2) $存在，則稱$ E((X - E(X))^2) $為$ X $的方差，記為$ Var(X) $。很顯然，離散型隨機變量$ X $的方差為$ Var(X) = \sum_{i=1}^{\infty} [x_i - E(X)]^2p_i$，連續型隨機變量$ X $的方差為$ Var(X) = \int_{-\infty}^{\infty} [x - E(X)]^2f(x)dx $。

8. 期望與方差的性質：

    - 對於任意兩個隨機變量$ X_1 $和$ X_2 $，則有$ E(X_1 + X_2) = E(X_1) + E(X_2) $。
    - 若$ X $是隨機變量，$ a $和$ b $是任意常量，則有$ E(aX + b) = aE(X) + b $和$ Var(aX + b) = a^2Var(X)$。
    - 若隨機變量$ X_1 $和$ X_2 $獨立，則有$ Var(X_1 + X_2) = Var(X_1) + Var(X_2) $。

9. 其他零碎小概念：

    - 互斥（*mutually exclusive*）：事件不能同時發生。
    - 獨立（*independant*）：一個試驗的結果不會對另一個試驗的結果產生影響。
    - 排列（permutation）：$ P_k^n = \frac{n!}{(n - k)!} $，國內教科書一般記為$ P_n^k $或$ A_n^k $。
    - 組合（*combination*）：$ C_k^n = \frac{n!}{k!(n-k)!} $，國內教科書一般記為$ C_n^k $。

#### 離散型分布

1. 伯努利分布（*Bernoulli distribution*）：又名**兩點分布**或者**0-1分布**，是一個離散型概率分布。若伯努利試驗成功，則隨機變量取值為1。若伯努利試驗失敗，則隨機變量取值為0。記其成功概率為$ p (0 \le p \le 1) $，失敗概率為$ q=1-p $，則概率質量函數為：

    $$ f(x)=p^{x}(1-p)^{1-x}= \left
    \{\begin{matrix}p&{if }x=1,\\ q &{if }x=0.\\ \end{matrix} \right. $$

2. 二項分布（*Binomial distribution*）：$n$個獨立的是/非試驗中成功次數的離散概率分布，其中每次試驗的成功概率為$p$。一般地，如果隨機變量$X$服從參數為$ n $和$ p $的二項分布，記為$ X\sim B(n,p) $。$ n $次試驗中正好得到$ k $次成功的概率由概率質量函數給出，
    $$ P(X=k) = C_k^np^k(1-p)^{n-k} $$

    > **提示**：Excel 中，可以通過`BINOM.DIST.RANGE`函數計算二項分布的概率。

3. 泊松分布（*Poisson distribution*）：適合於描述單位時間內隨機事件發生的次數的概率分布。如某一服務設施在一定時間內受到的服務請求的次數、汽車站台的候客人數、機器出現的故障數、自然災害發生的次數、DNA序列的變異數、放射性原子核的衰變數等等。泊松分布的概率質量函數為：$P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!}$，泊松分布的參數$\lambda$是單位時間（或單位面積）內隨機事件的平均發生率。

    > **說明**：泊松分布是在沒有計算機的年代，由於二項分布的運算量太大運算比較困難，為了減少運算量，數學家為二項分布提供的一種近似。當二項分布的$n$很大，$p$很小的時候，我們可以讓$\lambda = np$，然後用泊松分布的概率質量函數計算概率來近似二項分布的概率。

#### 分布函數

對於連續型隨機變量，我們不可能去羅列每一個值出現的概率，因此要引入分布函數的概念。
$$
F(x) = P\{X \le x\}
$$
如果將$ X $看成是數軸上的隨機坐標，上面的分布函數表示了$ x $落在區間$ (-\infty, x) $中的概率。分布函數有以下性質：

1. $ F(x) $是一個單調不減的函數；
2. $ 0 \le F(x) \le 1$，且$ F(-\infty) = \lim_{x \to -\infty} F(x) = 0 $， $F(\infty) = \lim_{x \to \infty} F(x) = 1$；
3. $ F(x) $是右連續的。

概率密度函數就是給分布函數求導的結果，簡單的說就是：
$$
F(x) = \int_{- \infty}^{x} f(t)dt
$$

#### 連續型分布

1. 均勻分布（*Uniform distribution*）：如果連續型隨機變量$X$具有概率密度函數$f(x)=\begin{cases}{\frac{1}{b-a}} \quad &{a \leq x \leq b} \\ {0} \quad &{other}\end{cases}$，則稱$X$服從$[a,b]$上的均勻分布，記作$X\sim U[a,b]$。

2. 指數分布（*Exponential distribution*）：如果連續型隨機變量$X$具有概率密度函數$f(x)=\begin{cases} \lambda e^{- \lambda x} \quad &{x \ge 0} \\ {0} \quad &{x \lt 0} \end{cases}$，則稱$X$服從參數為$\lambda$的指數分布，記為$X \sim Exp(\lambda)$。指數分布可以用來表示獨立隨機事件發生的時間間隔，比如旅客進入機場的時間間隔、客服中心接入電話的時間間隔、知乎上出現新問題的時間間隔等等。指數分布的一個重要特征是無記憶性（無後效性），這表示如果一個隨機變量呈指數分布，它的條件概率遵循：$P(T \gt s+t\ |\ T \gt t)=P(T \gt s), \forall s,t \ge 0$。

3. 正態分布（*Normal distribution*）：又名**高斯分布**（*Gaussian distribution*），是一個非常常見的連續概率分布，經常用自然科學和社會科學中來代表一個不明的隨機變量。若隨機變量$X$服從一個位置參數為$\mu$、尺度參數為$\sigma$的正態分布，記為$X \sim N(\mu,\sigma^2)$，其概率密度函數為：$f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}(\frac{x- \mu}{\sigma})}$。

    根據“棣莫弗-拉普拉斯積分定理”，假設$ \mu_{n} (n=1, 2, \cdots) $表示$ n $重伯努利試驗中成功的次數，已知每次試驗成功的概率為$p$，那麽：
    $$ \lim_{n \to \infty} P \lbrace \frac{\mu_n - np} {\sqrt{np(1-p)}} \le x \rbrace = \frac {1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{\mu^2}{2}}dx $$，該定理表明正態分布是二項分布的極限分布。
    

提到正態分布，就必須說一下“3$\sigma$法則”，該法則也稱為“68-95-99.7”法則，如下圖所示。

<img src="https://github.com/jackfrued/mypic/raw/master/20210716155542.png" style="zoom:65%">

正態分布有一個非常重要的性質，**大量統計獨立的隨機變量的平均值的分布趨於正態分布**，即$ \bar{X} \sim N(\mu, \frac{\sigma^2}{n}) $這就是**中心極限定理**。中心極限定理的重要意義在於，我們可以用正態分布作為其他概率分布的近似。

一個例子：假設某校入學新生的智力測驗平均分數與標準差分別為 100 與 12。那麽隨機抽取 50 個學生，他們智力測驗平均分數大於 105 的概率是多少？小於 90 的概率是多少？

本例沒有正態分布的假設，還好中心極限定理提供一個可行解，那就是當隨機樣本數量超過30，樣本平均數近似於一個正態變量，我們可以構造標準正態變量$ Z = \frac {\bar{X} - \mu} {\sigma / \sqrt{n}} $。

平均分數大於 105 的概率為：$ P(Z \gt \frac{105 - 100}{12 / \sqrt{50}}) = P(Z \gt 5/1.7) = P(Z \gt 2.94) = 0.0016$。

平均分數小於 90 的概率為：$ P(Z \lt \frac{90-100}{12/\sqrt{50}}) = P(Z < -5.88) = 0.0000 $。
    

> **說明**：上面標準正態分布的概率值可以查表得到，在 Excel 中可以使用`NORM.DIST`函數獲得。例如在上面的例子中，我們可以通過`NORM.DIST(2.94, 0, 1, TRUE)`獲得$P(z\le2.94)$的概率為`0.998359`。

#### 基於正態分布的三大分布

1. 卡方分布（*Chi-square distribution*）：若$k$個隨機變量$Z_1,Z_2,...,Z_k$是相互獨立且服從標準正態分布$N(0, 1)$的隨機變量，則隨機變量$X = \sum_{i=1}^{k}Z_i^2$被稱為服從自由度為$k$的卡方分布，記為$X \sim \chi^2(k)$。卡方分布的概率密度曲線如下所示。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220323201608.png" style="zoom:50%;">

2. $t$分布：設$X \sim N(0, 1)$， $Y \sim {\chi}^2(n)$，且$X$與$Y$相互獨立，則隨機變量$T = \frac {X} {\sqrt{Y/n}}$稱為自由度為$n$的$t$分布，記作$T \sim t(n)$。$t$分布的概率密度曲線如下所示。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220323203530.png" style="zoom:50%">

3. $F$分布：設$X \sim \chi^2(n_1)$，$Y \sim \chi^2(n_2)$，且$X$與$Y$相互獨立，則隨機變量$F = \frac{X / n_1}{Y / n_2}$稱為自由度為$(n_1, n_2)$的$F$分布，記作$F \sim F(n_1, n_2)$，它的概率密度曲線如下所示。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220619164716.png" style="zoom: 50%;">

這三個分布有什麽用呢？

1. $ \chi^2 $分布：常用於獨立性檢驗、擬合優度檢驗。
2. $ F $分布：常用於比例的估計和檢驗，方差分析和回歸分析中也會用到$ F $分布。
3. $ t $分布：在信息不足的情況下，要對總體均值進行估計和檢驗，就會使用到$ t $分布。

### 其他內容

#### 貝葉斯定理

**聯合概率**是指事件A和事件B共同發生的概率，通常記為$\small{P(A \cap B)}$。

**條件概率**是指事件A在事件B發生的條件下發生的概率，通常記為$\small{P(A|B)}$。設A與B為樣本空間$\Omega$中的兩個事件，其中$\small{P(B) \gt 0}$。那麽在事件B發生的條件下，事件A發生的條件概率為：${P(A|B)=\frac{P(A \cap B)}{P(B)}}$，當$ P(B)=0 $時，規定$ P(A|B) = 0 $。

> **思考**：
>
> 1. 某家庭有兩個孩子，問兩個孩子都是女孩的概率是多少？
> 2. 某家庭有兩個孩子，已知其中一個是女孩，問兩個孩子都是女孩的概率是多少？
> 3. 某家庭有兩個孩子，已知老大是女孩，問兩個孩子都是女孩的概率是多少？

事件A在事件B已發生的條件下發生的概率，與事件B在事件A已發生的條件下發生的概率是不一樣的。然而，這兩者是有確定的關系的，**貝葉斯定理**就是對這種關系的陳述，如下所示：
$$
P(A|B)=\frac{P(B|A)}{P(B)}P(A)
$$

- $P(A|B)$是已知$B$發生後，$A$的條件概率，也稱為$A$的後驗概率。
- $P(A)$是$A$的先驗概率也稱作邊緣概率，是不考慮$B$時$A$發生的概率。
- $P(B|A)$是已知$A$發生後，$B$的條件概率，稱為$B$的似然性。
- $P(B)$是$B$的先驗概率。

#### 大數定理

樣本數量越多，則其算術平均值就有越高的概率接近期望值。

1. 弱大數定律（辛欽定理）：樣本均值依概率收斂於期望值，即對於任意正數$\epsilon$，有：$\lim_{n \to \infty}P(|\bar{X_n}-\mu|>\epsilon)=0$。
2. 強大數定律：樣本均值以概率1收斂於期望值，即：$P(\lim_{n \to \infty}\bar{X_n}=\mu)=1$。

#### 假設檢驗

假設檢驗就是通過抽取樣本數據，並且通過**小概率反證法**去驗證整體情況的方法。假設檢驗的核心思想是小概率反證法（首先假設想推翻的命題是成立的，然後試圖找出矛盾，找出不合理的地方來證明命題為假命題），即在**零假設**（通常記為$H_0$）的前提下，估算某事件發生的可能性，如果該事件是小概率事件，在一次試驗中本不應該發生，但現在卻發生了，此時我們就有足夠的理由懷疑零假設，轉而接受**備擇假設**（通常記為$H_A$）。

假設檢驗會存在兩種錯誤情況，一種稱為“拒真”，一種稱為“取偽”。如果原假設是對的，但你拒絕了原假設，這種錯誤就叫作“拒真”，這個錯誤的概率也叫作顯著性水平$\alpha$，或稱為容忍度；如果原假設是錯的，但你承認了原假設，這種錯誤就叫作“取偽”，這個錯誤的概率我們記為$\beta$。

### 總結

描述性統計通常用於研究表象，將現象用數據的方式描述出來（用整體的數據來描述整體的特征）；推理性統計通常用於推測本質（通過樣本數據特征去推理總體數據特征），也就是你看到的表象的東西有多大概率符合你對隱藏在表象後的本質的猜測。
